{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose \n",
    "\n",
    "the purpose of this notebook is to demonstrate the working of the from scratch implementation of a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'metrics' from 'c:\\\\Users\\\\Aryan\\\\Desktop\\\\Assignment 1 ML\\\\metrics.py'>\n"
     ]
    }
   ],
   "source": [
    "print(mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the accuracy.\n",
    "    \"\"\"\n",
    "    # Assert that y_hat and y have the same length\n",
    "    assert y_hat.size == y.size, \"Size of y_hat and y must be equal.\"\n",
    "\n",
    "    \n",
    "    numerator = (y_hat == y).sum()\n",
    "    #print(f\"Numerator (correct predictions): {numerator}\")\n",
    "\n",
    "   \n",
    "    denominator = y.size\n",
    "    #print(f\"Denominator (total predictions): {denominator}\")\n",
    "\n",
    "    \n",
    "    accuracy = numerator / denominator\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Define y_hat and y\n",
    "y_hat = pd.Series(['yes', 1, 1,1])\n",
    "y = pd.Series(['yes', 1, 1,1])\n",
    "\n",
    "# Correctly compute accuracy using the function from mn\n",
    "accuracy_result = accuracy(y_hat, y)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Accuracy: {accuracy_result}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Variations of the problem:\n",
    "1) Discrete I/P, Discrete O/P => classficiation \n",
    "2) Discrete I/P, Real O/P => regression \n",
    "3) Real I/P, Discrete O/P => classification\n",
    "3) Real I/P, Real O/P => regression\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the precision. Defined only for 1) and 3). \n",
    "    \"\"\"\n",
    "    true_positives = ((y_hat == cls) & (y == cls)).sum()\n",
    "    \n",
    "    # Calculate predicted positives\n",
    "    gnd_positives = (y == cls).sum()\n",
    "    \n",
    "    return true_positives / gnd_positives if gnd_positives != 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = pd.Series(['yes', 1, 1, 1])\n",
    "y = pd.Series(['yes', 1, 0, 1])\n",
    "precision(y_hat, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_hat: pd.Series, y: pd.Series, cls: Union[int, str]) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the precision. Defined only for 1) and 3). \n",
    "    \"\"\"\n",
    "    true_positives = ((y_hat == cls) & (y == cls)).sum()\n",
    "    \n",
    "    # Calculate predicted positives\n",
    "    gnd_positives = (y == cls).sum()\n",
    "    \n",
    "    return true_positives / gnd_positives if gnd_positives != 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = pd.Series(['yes', 1, 1, 1])\n",
    "y = pd.Series(['yes', 1, 0, 1])\n",
    "recall(y_hat, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the root-mean-squared-error(rmse). Defined only for 2) and 4).\n",
    "    \"\"\"\n",
    "\n",
    "    assert y_hat.size == y.size, \"Size of y_hat and y must be equal.\"\n",
    "\n",
    "    y_hat = np.array(y_hat)\n",
    "    y = np.array(y)\n",
    "    numerator = np.sum((y_hat-y)**2)\n",
    "    denominator = y.size\n",
    "\n",
    "    return np.sqrt(numerator/denominator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat =pd.Series([10,20])\n",
    "y = pd.Series([20,30])\n",
    "\n",
    "rmse(y_hat,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_hat: pd.Series, y: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Function to calculate the mean-absolute-error(mae). Defined only for 2) and 4).\n",
    "    \"\"\"\n",
    "    assert y_hat.size == y.size, \"Size of y_hat and y must be equal.\"\n",
    "    assert y.size, \"Ground Truth array is 0\"\n",
    "    assert y_hat.size, \"Predicition array is 0\"\n",
    "\n",
    "\n",
    "    y_hat = np.array(y_hat)\n",
    "    y = np.array(y)\n",
    "    numerator = np.sum((np.abs(y_hat-y)))\n",
    "\n",
    "    denominator = y.size\n",
    "\n",
    "    return numerator/denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat =pd.Series([11,20])\n",
    "y = pd.Series([20,30])\n",
    "\n",
    "mae(y_hat,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
